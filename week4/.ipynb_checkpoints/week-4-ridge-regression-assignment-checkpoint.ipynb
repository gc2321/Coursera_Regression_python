{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 4: Ridge Regression (interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build on the material from Week 3, where we wrote the function to produce an SFrame with columns containing the powers of a given input. Copy and paste the function `polynomial_sframe` from Week 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_dataframe(feature, degree): # feature is pandas.Series type\n",
    "    # assume that degree >= 1\n",
    "    # initialize the dataframe:\n",
    "    poly_dataframe = pd.DataFrame()\n",
    "    poly_dataframe['power_1'] = feature\n",
    "\n",
    "    if degree > 1:\n",
    "        # then loop over the remaining degrees:\n",
    "        for power in range(2, degree+1):\n",
    "            # first we'll give the column a name:\n",
    "            name = 'power_' + str(power)\n",
    "            poly_dataframe[name] = feature.apply(lambda x: x**power)\n",
    "\n",
    "    return poly_dataframe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n",
    "\n",
    "sales = pd.read_csv('kc_house_data.csv', dtype=dtype_dict)\n",
    "sales = sales.sort(['sqft_living','price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us revisit the 15th-order polynomial model using the 'sqft_living' input. Generate polynomial features up to degree 15 using `polynomial_sframe()` and fit a model with these features. When fitting the model, use an L2 penalty of 1.5e-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_small_penalty = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When we have so many features and so few data points, the solution can become highly numerically unstable, which can sometimes lead to strange unpredictable results. Thus, rather than using no regularization, we will introduce a tiny amount of regularization (l2_penalty=1.5e-5) to make the solution numerically stable. (In lecture, we discussed the fact that regularization can also help with numerical stability, and here we are seeing a practical example.)\n",
    "\n",
    "With the L2 penalty specified above, fit the model and print out the learned weights. Add \"alpha=l2_small_penalty\" and \"normalize=True\" to the parameter list of linear_model.Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1e-05, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=True, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "poly15_data = polynomial_dataframe(sales['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)\n",
    "model.fit(poly15_data, sales['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.74425503e+02,  -7.83644074e-02,   3.73280427e-05,\n",
       "        -2.85554311e-09,  -2.37487557e-13,   9.70529709e-18,\n",
       "         1.77791834e-21,   9.76993158e-26,   7.78897289e-31,\n",
       "        -3.83833667e-34,  -4.52940095e-38,  -3.16694117e-42,\n",
       "        -1.24102475e-46,   4.53456467e-51,   1.63425896e-54])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all coefficient\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTION:  What's the learned value for the coefficient of feature `power_1`?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Module 3 (Polynomial Regression) that the polynomial fit of degree 15 changed wildly whenever the data changed. In particular, when we split the sales data into four subsets and fit the model of degree 15, the result came out to be very different for each subset. The model had a high variance. We will see in a moment that ridge regression reduces such variance. But first, we must reproduce the experiment we did in Module 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, please download the provided csv files for each subset and load them with the given list of types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtype_dict same as above\n",
    "set_1 = pd.read_csv('wk3_kc_house_set_1_data.csv', dtype=dtype_dict)\n",
    "set_2 = pd.read_csv('wk3_kc_house_set_2_data.csv', dtype=dtype_dict)\n",
    "set_3 = pd.read_csv('wk3_kc_house_set_3_data.csv', dtype=dtype_dict)\n",
    "set_4 = pd.read_csv('wk3_kc_house_set_4_data.csv', dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in Module 3 (Polynomial Regression), fit a 15th degree polynomial on each of the 4 sets, plot the results and view the weights for the four models. This time, set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l2_small_penalty=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.44669415e+02,  -3.55447622e-01,   1.22446389e-04,\n",
       "        -1.17175330e-08,  -3.90512409e-13,  -1.39076037e-17,\n",
       "         1.47860278e-20,   6.87491658e-25,  -7.57203881e-29,\n",
       "        -1.04097302e-32,  -3.71844073e-37,   3.39989349e-41,\n",
       "         5.56591960e-45,   2.53761327e-49,  -3.35152872e-53])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_1\n",
    "poly15_data = polynomial_dataframe(set_1['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_1['price'])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "power1=[]\n",
    "power1.append(model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.59362621e+02,  -8.18118239e-01,   4.28879956e-04,\n",
       "        -9.12770557e-08,  -2.69604615e-12,   3.73980317e-15,\n",
       "        -1.42711857e-19,  -6.30794783e-23,  -1.44559533e-27,\n",
       "         7.44321284e-31,   9.25865970e-35,   3.27995343e-41,\n",
       "        -1.29543496e-42,  -1.38781257e-46,   1.66546442e-50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_2\n",
    "poly15_data = polynomial_dataframe(set_2['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_2['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.55395951e+02,   9.75579528e-01,  -4.58946002e-04,\n",
       "         7.77958122e-08,   7.15013291e-12,  -2.88601962e-15,\n",
       "        -2.13678316e-20,   3.38085242e-23,   2.19178184e-27,\n",
       "        -1.97067757e-31,  -4.15993156e-35,  -1.80196240e-39,\n",
       "         3.19071197e-43,   5.08456935e-47,  -3.93304278e-51])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_3\n",
    "poly15_data = polynomial_dataframe(set_3['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_3['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.11944571e+03,  -9.83760211e-01,   3.38770895e-04,\n",
       "         3.60377203e-08,  -4.37814001e-11,   5.77191610e-15,\n",
       "         7.66795376e-19,  -9.49297755e-23,  -1.96030822e-26,\n",
       "        -2.10881744e-32,   3.31005080e-34,   3.47733902e-38,\n",
       "        -2.43039275e-42,  -8.79553341e-46,   6.44569723e-50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_4\n",
    "poly15_data = polynomial_dataframe(set_4['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_small_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_4['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119.4457135107316, -755.39595093783441)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max, min of power1\n",
    "max(power1), min(power1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four curves should differ from one another a lot, as should the coefficients you learned.\n",
    "\n",
    "***QUIZ QUESTION:  For the models learned in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature `power_1`?***  (For the purpose of answering this question, negative numbers are considered \"smaller\" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression comes to rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, whenever we see weights change so much in response to change in data, we believe the variance of our estimate to be large. Ridge regression aims to address this issue by penalizing \"large\" weights. (Weights of `model15` looked quite small, but they are not that small because 'sqft_living' input is in the order of thousands.)\n",
    "\n",
    "With the argument `l2_penalty=1e5`, fit a 15th-order polynomial model on `set_1`, `set_2`, `set_3`, and `set_4`. Other than the change in the `l2_penalty` parameter, the code should be the same as the experiment above. Also, make sure GraphLab Create doesn't create its own validation set by using the option `validation_set = None` in this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l2_large_penalty=1.23e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "power1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.32806803e+00,   3.53621608e-04,   3.31969692e-08,\n",
       "         2.00082477e-12,   1.11492559e-16,   6.57786122e-21,\n",
       "         4.12939525e-25,   2.70393755e-29,   1.81614763e-33,\n",
       "         1.23824277e-37,   8.51872481e-42,   5.89455598e-46,\n",
       "         4.09542560e-50,   2.85464889e-54,   1.99547476e-58])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_1\n",
    "poly15_data = polynomial_dataframe(set_1['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_large_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_1['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.09756903e+00,   3.90817483e-04,   6.67189944e-08,\n",
       "         8.90002997e-12,   9.72639877e-16,   9.69733682e-20,\n",
       "         9.50564475e-24,   9.44491031e-28,   9.57191338e-32,\n",
       "         9.86945155e-36,   1.03101115e-39,   1.08729784e-43,\n",
       "         1.15453748e-47,   1.23211305e-51,   1.31986696e-55])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_2\n",
    "poly15_data = polynomial_dataframe(set_2['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_large_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_2['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.28906258e+00,   4.12472190e-04,   6.08835345e-08,\n",
       "         6.58572163e-12,   6.15278155e-16,   5.64446634e-20,\n",
       "         5.28834396e-24,   5.07091402e-28,   4.94657273e-32,\n",
       "         4.88043809e-36,   4.85009106e-40,   4.84161534e-44,\n",
       "         4.84635021e-48,   4.85883628e-52,   4.87558469e-56])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_3\n",
    "poly15_data = polynomial_dataframe(set_3['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_large_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_3['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.08596194e+00,   4.05035772e-04,   7.46864647e-08,\n",
       "         1.13096608e-11,   1.45864442e-15,   1.73561251e-19,\n",
       "         2.01609632e-23,   2.34605255e-27,   2.75636073e-31,\n",
       "         3.27043069e-35,   3.91046855e-39,   4.70118041e-43,\n",
       "         5.67212304e-47,   6.85958087e-51,   8.30843630e-55])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set_4\n",
    "poly15_data = polynomial_dataframe(set_4['sqft_living'], 15)\n",
    "model = linear_model.Ridge(alpha=l2_large_penalty, normalize=True)\n",
    "model.fit(poly15_data, set_4['price'])\n",
    "power1.append(model.coef_[0])\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3280680295793239, 2.0859619409193062)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max, min of power1\n",
    "max(power1), min(power1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These curves should vary a lot less, now that you applied a high degree of regularization.\n",
    "\n",
    "***QUIZ QUESTION:  For the models learned with the high level of regularization in each of these training sets, what are the smallest and largest values you learned for the coefficient of feature `power_1`?*** (For the purpose of answering this question, negative numbers are considered \"smaller\" than positive numbers. So -5 is smaller than -3, and -3 is smaller than 5 and so forth.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting an L2 penalty via cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the polynomial degree, the L2 penalty is a \"magic\" parameter we need to select. We could use the validation set approach as we did in the last module, but that approach has a major disadvantage: it leaves fewer observations available for training. **Cross-validation** seeks to overcome this issue by using all of the training set in a smart way.\n",
    "\n",
    "We will implement a kind of cross-validation called **k-fold cross-validation**. The method gets its name because it involves dividing the training set into k segments of roughtly equal size. Similar to the validation set method, we measure the validation error with one of the segments designated as the validation set. The major difference is that we repeat the process k times as follows:\n",
    "\n",
    "Set aside segment 0 as the validation set, and fit a model on rest of data, and evalutate it on this validation set<br>\n",
    "Set aside segment 1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set<br>\n",
    "...<br>\n",
    "Set aside segment k-1 as the validation set, and fit a model on rest of data, and evalutate it on this validation set\n",
    "\n",
    "After this process, we compute the average of the k validation errors, and use it as an estimate of the generalization error. Notice that  all observations are used for both training and validation, as we iterate over segments of data. \n",
    "\n",
    "To estimate the generalization error well, it is crucial to shuffle the training data before dividing them into segments. We reserve 10% of the data as the test set and randomly shuffle the remainder. Le'ts call the shuffled data 'train_valid_shuffled'.\n",
    "\n",
    "For the purpose of this assignment, let us download the csv file containing pre-shuffled rows of training and validation sets combined: wk3_kc_house_train_valid_shuffled.csv. In practice, you would shuffle the rows with a dynamically determined random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_valid_shuffled = pd.read_csv('wk3_kc_house_train_valid_shuffled.csv', dtype=dtype_dict)\n",
    "test = pd.read_csv('wk3_kc_house_test_data.csv', dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the combined training and validation set into equal segments. Each segment should receive n/k elements, where n is the number of observations in the training set and k is the number of segments. Since the segment 0 starts at index 0 and contains n/k elements, it ends at index (n/k)-1. The segment 1 starts where the segment 0 left off, at index (n/k). With n/k elements, the segment 1 ends at index (n*2/k)-1. Continuing in this fashion, we deduce that the segment i starts at index (n*i/k) and ends at (n*(i+1)/k)-1.\n",
    "\n",
    "With this pattern in mind, we write a short loop that prints the starting and ending indices of each segment, just to make sure you are getting the splits right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0, 1938)\n",
      "1 (1939, 3878)\n",
      "2 (3879, 5817)\n",
      "3 (5818, 7757)\n",
      "4 (7758, 9697)\n",
      "5 (9698, 11636)\n",
      "6 (11637, 13576)\n",
      "7 (13577, 15515)\n",
      "8 (15516, 17455)\n",
      "9 (17456, 19395)\n"
     ]
    }
   ],
   "source": [
    "n = len(train_valid_shuffled)\n",
    "k = 10 # 10-fold cross-validation\n",
    "\n",
    "for i in xrange(k):\n",
    "    start = (n*i)/k\n",
    "    end = (n*(i+1))/k-1\n",
    "    print i, (start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us familiarize ourselves with array slicing with Pandas. To extract a continuous slice from a DataFrame, use colon in square brackets. For instance, the following cell extracts rows 0 to 9 of train_valid_shuffled. Notice that the first index (0) is included in the slice but the last index (10) is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2780400035</td>\n",
       "      <td>20140505T000000</td>\n",
       "      <td>665000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2800</td>\n",
       "      <td>5900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1660</td>\n",
       "      <td>1140</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6809</td>\n",
       "      <td>-122.286</td>\n",
       "      <td>2580</td>\n",
       "      <td>5900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1703050500</td>\n",
       "      <td>20150321T000000</td>\n",
       "      <td>645000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2490</td>\n",
       "      <td>5978</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2490</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6298</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2710</td>\n",
       "      <td>6629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5700002325</td>\n",
       "      <td>20140605T000000</td>\n",
       "      <td>640000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2340</td>\n",
       "      <td>4206</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1170</td>\n",
       "      <td>1170</td>\n",
       "      <td>1917</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5759</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1360</td>\n",
       "      <td>4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0475000510</td>\n",
       "      <td>20141118T000000</td>\n",
       "      <td>594000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1320</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1090</td>\n",
       "      <td>230</td>\n",
       "      <td>1920</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6674</td>\n",
       "      <td>-122.365</td>\n",
       "      <td>1700</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0844001052</td>\n",
       "      <td>20150128T000000</td>\n",
       "      <td>365000</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1904</td>\n",
       "      <td>8200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1904</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3107</td>\n",
       "      <td>-122.001</td>\n",
       "      <td>1560</td>\n",
       "      <td>12426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2781280290</td>\n",
       "      <td>20150427T000000</td>\n",
       "      <td>305000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1610</td>\n",
       "      <td>3516</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1610</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98055</td>\n",
       "      <td>47.4491</td>\n",
       "      <td>-122.188</td>\n",
       "      <td>1610</td>\n",
       "      <td>3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2214800630</td>\n",
       "      <td>20141105T000000</td>\n",
       "      <td>239950</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1560</td>\n",
       "      <td>8280</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1560</td>\n",
       "      <td>0</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.3393</td>\n",
       "      <td>-122.259</td>\n",
       "      <td>1920</td>\n",
       "      <td>8120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2114700540</td>\n",
       "      <td>20141021T000000</td>\n",
       "      <td>366000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1320</td>\n",
       "      <td>4320</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>660</td>\n",
       "      <td>660</td>\n",
       "      <td>1918</td>\n",
       "      <td>0</td>\n",
       "      <td>98106</td>\n",
       "      <td>47.5327</td>\n",
       "      <td>-122.347</td>\n",
       "      <td>1190</td>\n",
       "      <td>4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2596400050</td>\n",
       "      <td>20140730T000000</td>\n",
       "      <td>375000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>7955</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1260</td>\n",
       "      <td>700</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98177</td>\n",
       "      <td>47.7641</td>\n",
       "      <td>-122.364</td>\n",
       "      <td>1850</td>\n",
       "      <td>8219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4140900050</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>440000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2180</td>\n",
       "      <td>10200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2000</td>\n",
       "      <td>180</td>\n",
       "      <td>1966</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7638</td>\n",
       "      <td>-122.270</td>\n",
       "      <td>2590</td>\n",
       "      <td>10445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date   price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  2780400035  20140505T000000  665000         4       2.50         2800   \n",
       "1  1703050500  20150321T000000  645000         3       2.50         2490   \n",
       "2  5700002325  20140605T000000  640000         3       1.75         2340   \n",
       "3  0475000510  20141118T000000  594000         3       1.00         1320   \n",
       "4  0844001052  20150128T000000  365000         4       2.50         1904   \n",
       "5  2781280290  20150427T000000  305000         3       2.50         1610   \n",
       "6  2214800630  20141105T000000  239950         3       2.25         1560   \n",
       "7  2114700540  20141021T000000  366000         3       2.50         1320   \n",
       "8  2596400050  20140730T000000  375000         3       1.00         1960   \n",
       "9  4140900050  20150126T000000  440000         4       1.75         2180   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5900       1           0     0     ...          8        1660   \n",
       "1      5978       2           0     0     ...          9        2490   \n",
       "2      4206       1           0     0     ...          7        1170   \n",
       "3      5000       1           0     0     ...          7        1090   \n",
       "4      8200       2           0     0     ...          7        1904   \n",
       "5      3516       2           0     0     ...          8        1610   \n",
       "6      8280       2           0     0     ...          7        1560   \n",
       "7      4320       1           0     0     ...          6         660   \n",
       "8      7955       1           0     0     ...          7        1260   \n",
       "9     10200       1           0     2     ...          8        2000   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0           1140      1963             0    98115  47.6809 -122.286   \n",
       "1              0      2003             0    98074  47.6298 -122.022   \n",
       "2           1170      1917             0    98144  47.5759 -122.288   \n",
       "3            230      1920             0    98107  47.6674 -122.365   \n",
       "4              0      1999             0    98010  47.3107 -122.001   \n",
       "5              0      2006             0    98055  47.4491 -122.188   \n",
       "6              0      1979             0    98001  47.3393 -122.259   \n",
       "7            660      1918             0    98106  47.5327 -122.347   \n",
       "8            700      1963             0    98177  47.7641 -122.364   \n",
       "9            180      1966             0    98028  47.7638 -122.270   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           2580        5900  \n",
       "1           2710        6629  \n",
       "2           1360        4725  \n",
       "3           1700        5000  \n",
       "4           1560       12426  \n",
       "5           1610        3056  \n",
       "6           1920        8120  \n",
       "7           1190        4200  \n",
       "8           1850        8219  \n",
       "9           2590       10445  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_shuffled[0:10] # rows 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the observations are grouped into 10 segments, the segment i is given by\n",
    "\n",
    "    start = (n*i)/10\n",
    "\n",
    "    end = (n*(i+1))/10\n",
    "\n",
    "    train_valid_shuffled[start:end+1]\n",
    "\n",
    "Meanwhile, to choose the remainder of the data that's not part of the segment i, we select two slices (0:start) and (end+1:n) and paste them together.\n",
    "\n",
    "    train_valid_shuffled[0:start].append(train_valid_shuffled[end+1:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we are ready to implement k-fold cross-validation. Write a function that computes k validation errors by designating each of the k segments as the validation set. It accepts as parameters (i) k, (ii) l2_penalty, (iii) dataframe containing input features (e.g. poly15_data) and (iv) column of output values (e.g. price). The function returns the average validation error using k segments as validation sets. We shall assume that the input dataframe does not contain the output column.\n",
    "\n",
    "For each i in [0, 1, ... k-1]:\n",
    "\n",
    "    Compute starting and ending indices of segment i and call 'start' and 'end'\n",
    "    Form validation set by taking a slice (start:end+1) from the data.\n",
    "    Form training set by appending slice (end+1:n) to the end of slice (0:start).\n",
    "    Train a linear model using training set just formed, with a given l2_penalty\n",
    "    Compute validation error (RSS) using validation set just formed\n",
    "\n",
    "e.g. in Python:\n",
    "\n",
    "    def k_fold_cross_validation(k, l2_penalty, data, output):\n",
    "        ...\n",
    "        return [average validation error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(k, l2_penalty, data, output_name, features_list):\n",
    "    \n",
    "    # generate segments\n",
    "    seg = []\n",
    "    for i in xrange(k):\n",
    "        start = (n*i)/k\n",
    "        end = (n*(i+1))/k-1\n",
    "        seg.append((start, end))\n",
    "    \n",
    "    for each in seg:\n",
    "        start = each[0]\n",
    "        end = each[1]\n",
    "        n = len(data)\n",
    "        validate = data[start:end+1]\n",
    "        train = data[0:start].append(data[end+1:n])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a function to compute the average validation error for a model, we can write a loop to find the model that minimizes the average validation error. Write a loop that does the following:\n",
    "* We will again be aiming to fit a 15th-order polynomial model using the `sqft_living` input\n",
    "* For `l2_penalty` in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7] (to get this in Python, you can use this Numpy function: `np.logspace(1, 7, num=13)`.)\n",
    "    * Run 10-fold cross-validation with `l2_penalty`\n",
    "* Report which L2 penalty produced the lowest average validation error.\n",
    "\n",
    "Note: since the degree of the polynomial is now fixed to 15, to make things faster, you should generate polynomial features in advance and re-use them throughout the loop. Make sure to use `train_valid_shuffled` when generating polynomial features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS:  What is the best value for the L2 penalty according to 10-fold validation?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it useful to plot the k-fold cross-validation errors you have obtained to better understand the behavior of the method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the l2_penalty values in the x axis and the cross-validation error in the y axis.\n",
    "# Using plt.xscale('log') will make your plot more intuitive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you found the best value for the L2 penalty using cross-validation, it is important to retrain a final model on all of the training data using this value of `l2_penalty`.  This way, your final model will be trained on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTION: Using the best L2 penalty found above, train a model using all training data. What is the RSS on the TEST data of the model you learn with this L2 penalty? ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
